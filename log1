EVAN LEE'S LOG (All time converted to ET)
ESTIMATE of time to complete assignment: 15 hours

      Time     Time
Date  Started  Spent   Work completed
----  -------  ----    --------------
12/5 10;30am   1:30       Picked game, understood rules, played a few games, and discussed initial plans for agents
12/18 10:00pm  4:30  familiarized myself with game code, discussed with Nick about what we would implement, then left his dorm and worked solo to build random policy agent
12/19  2:00pm  3:30  fixed bug (ended up being local Python version) and thought about heuristics / strategies and started building heuristic agent. After adding only one rule, 100-0’d random agent and 10-0’d MCTS agent at that time (shuttle to airport)
12/19  7:30pm  5:00 built stronger and stronger heuristic agent via adding strategies and rules(on plane after takeoff and before landing)
12/20  4:00am  2:30 fixed bug(s) with discard list, separated into two baseline agents: simple greedy agent and heuristic agent, started overnight tests (after got home from airport)
12/20  11:45am 2:15 finished logs, did write-up, and cleaned up everqything for submission (next morning)
               ----
               19:15  TOTAL time spent

DISCUSSION

RANDOM POLICY DISCUSSION
At first, we implemented a random policy so that we could use it as a sanity check to make sure our initial MCTS and baseline agents were wokring properly. The random policy agent itself was not used in the final tests, but can be seen as Agent 0.

As we talked about in our video, our first baseline agent is a relatively simple greedy policy agent. In particular, it's a simple greedy policy that:
    1. Draws from discard pile if it allows itself to meld
    2. Melds if possible
    3. Avoids discarding cards that are part of a run or set draw (two consecutive, same-suit cards that could form a run or two of a kind)
    """ 
Essentially, it will leap at any immediate reward (e.g. getting a run or a set)

HEURISTIC AGENT DISCUSSION
INITIAL HEURISTIC AGENT
Result: Was badly beating both random MCTS implementantions at that point in time
The bulk of the work on the baseline agents went into the heuristic agent. After a simple rule only for discard was implemented that ranked cards by discard desirability, it was beating both the random and current MCTS implementations. 

This rule was revolved around the idea that you generally want to discard cards that are higher in value, all else being equal, in order to minimize the points that you give up in the case that you lose. More specifically, although all face cards and 10 are worth the same, we ranked K to be the highest to discard, followed by Q, followed by J and 10. This is because K and Q mathematically fit into less runs and so discarding those reduces the likelihood that your opponent finds your discard card useful. The rest of the ranking was 9,8,...,1 because of the respective points they corresponded to for your opponent in the case of a loss.

The initial plan was to define when discard options were 'not equal' and then use this rule when the discard options were equal (aka none being clearly better discards than the other). However, it ended up that this rule overpowered. It's likely due to the specific version of Rummy that we decided to focus on where you're forced to meld if you have the ability to do so. But this makes sense because the strategy doesn't have a clearly dominating response. The opponent can't just decide to collect the high cards because then they would be giving up a lot of points if they lose. If they decide to use a similar prefer-low-card algorithm, they're only equal.


HEURISTIC AGENT V2
Result: KO'd the intial heuristic agent (~100% win rate)

The main reason for this improvement was a draw rule that said, all else equal, to draw from the discard pile if the card was 6 or less in value and to draw from the stock pile otherwise. The 6 came from the fact that the sum of the values in any suit is 85 (1+2+3+...+10+10+10+10) and so the average card value that you can expect from the stock pile given no other information is roughly 6.5. And so to minimize the value of the card you drew you'd wamt to draw from the discard pile if the card was 6 or less in value and to draw from the stock pile otherwise.

The other new rules were improving the agent as well, but we wanted better as we had the goal of having each heuristic agent version completely thrash the previous heuristic agent version. So we experimented more creatively to push the boundaries of our agent.

This was one of those rules that was not expected to work. It was more or less tried as a curiosity since the idea of getting rid of lower cards was working way better than expected for the draw policy. 

It's not as intuitive of a rule because players typically use the draw pile to try to make sets or runs. A typical strategy, for instance, might be taking the discard pile card if it gives them two cards of the same suit in a row or a pair and taking a card from the stock pile otherwise. 

But our results seem to point to the fact that in doing, you may pick and hold onto high cards that may work against you more than it may help you because high cards are extra points for your opponents in the case that you lose the round.

After this agent was built, we had to add a non-determinism element or else two of these agents playing each other would sometmes result in both agents taking a card from the discard pile and then discarding that same card (which is allowed by game rules) and ending up in an infinite loop. This non-determinism also helps to reduce the overall predictability of the agent, making it harder to play against because other agents would have to screen out the noise from the non-determinism.


HEURISTIC AGENT V3
Result: KO'd v2 of the heuristic agent (~100% win rate)

We experimented with more rules and added and tuned specific parameters to optimize our agent. For instance, a rule was added that if opponent has more than 3 cards in hand (they're not close to winning), don't discard cards that contribute to a run or set draw where we're defining run draws to be two consecutive cards of the same suit and a set draw to be a pair.

The idea is that you discard higher value cards to minimize score you give opponent in case that you lose. But if the opponent doesn't seem close to winning, which is well-correlated with the number of cards they have left since melding is required if possible, then we can wait and see if we can make a run or set with higher rank cards.


Heuristic Agent V4 (FINAL VERSION)
Result: Beat v3 of heuristic agent (~54% win rate) via use of discard list

One piece of information that the previous heuristic agents weren't utilizing yet was the discard pile. There's no easy way to access the discard pile as you're not allowed to look at any cards in the discard pile besides the first card by the game rules. So this agent tracked the discard pile by keeping track that it updates after its own moves but also after opponent moves by looking at changes in the size of the stock pile. The logic was difficult to work and reason out and we ran into issues like duplicate cards in the discard pile list and not clearing the list at the right times before we fixed them.

Any uses of discard pile for trying to figure out what to discard were overpowered by the card ranking rule, so we turned to the draw strategy. Initially, the uses of the discard pile was also overpowered by the rule of choosing the card that had a lower value EV. So instead of trying to fit this rule, we used our new information to buff up this rule even more. In particular, we used the information of our hand, the cards that were melded, and the cards in the discard pile. The cards in the 52-card deck but not in those piles are probabilistically equally likely to be drawn from the stock pile, so the EV of a stock pile draw was calculated. Ultimately, the agent would pick the option with the lower expected rank value (EV from the stock pile and exact value from top card of discard pile). The heuristic agent was already pretty strong at this point, so the improvement was more marginal than the improvement in previous version jumps.

RESULTS DISCUSSION
We left our agents running against each other overnight. The MCTS agents were given 0.1 seconds per move. Here were the matchups, the results (proportion of games won by the respective agent), and the # of iterations:

Simple Greedy Base vs Base MCTS: [0.18107173725151254, 0.8189282627484875], 1157 iterations

Heuristic Base vs Base MCTS: [0.9976179132920439, 0.0023820867079561697], 2099 iterations

Simple Greedy Base vs. Advanced MCTS: [0.143317230273752, 0.856682769726248], 621 iterations

Heuristic Base vs Advanced MCTS: [0.9981325863678805, 0.0018674136321195146], 1071 iterations

Even though the MCTS agents don't work with much game information and conducts random playouts (and Simple Greedy Base is not a random policy), the MCTS agents still beat the Simple Greedy Base pretty easily. This shows the power of Monte Carlo simulations in informing game strategy. Even though it doesn't use much information and doesn't at all know the opponent's strategy, it still gives a strong indication of what moves are good and what moves aren't.

In our tests with Base MCTS against Advanced MCTS, it was the case that Advanced MCTS was stronger. This was confirmed by the results shown above. In particular, the Base MCTS won ~82% of games against the Simple Greedy Base, while the Advanced MCTS won ~86% of games against the Simple Greedy Base (86% > 82%). 

Both MCTS agents were beaten pretty badly by the Heuristic Base, which is reasonable since the MCTS agents are doing random playouts with little game specific information and the Heuristic Base uses very specific rules and strategies with knowledge and careful human analysis of the game. Given more time, we could look at the MCTS agents using heuristic-based policies for playouts. This would likely beat the Heuristic Base and be the strongest set of agents yet.