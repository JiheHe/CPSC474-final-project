Interesting observation:
If we don't switch non-dealer (starter) position, then the starter has 60% win rate in random vs. random scenario, signaling that
it is a favored position under the current rule. With the alternating implemented, the dealers have even win rates. The setup is now fair.

Also confirmed that the game works as intended. None of the errors are raised, signaling proper agent implementations following the game
rules. Also did some print statements to inspect the output to follow through some games; all seem to make sense logically.

For MCTS, had to interweave between the policy game instance and the MCTS state transition which is something new. Had to come up with 'hacks' and state shaping that transitions from an imperfect information game into a perfect sceneario capable of MCTS. i.e. random assumption of unseen cards to mask information (directly using game would be perfect information, violating the reality of the game), reward shaping (reasonable), single-player, granularity of states, etc. Much easier with draw_policy currently since only 2 actions all the time, and runtime-wise more friendly.
Single-player works for discard policy too, reasonably assuming so.

MCTS slow due to the fact that we have to deepcopy lists of cards for every game states at every node, in order for the references to not mess up.
And the uncertainty number of turns during a playout introduces high variance. On average, it takes about 5 seconds for the MCTS agent (settings below) to compute one game against a Random agent, currently.
Even with base MCTS on draw_policy only and random discard_policy, and 0.01s compute time for MCTS, and random assumption of unknown cards,
random agent is getting destroyed 90% of the time.

Potential: expand the scope of meld to include full rule, and apply MCTS to discard policy too. (RIP computer; runtime concern)


Observation: after draw-only, new card isn't laid on top. So if we naively call draw next turn, then the discard[-1] errors. We could
add a None condition to cover this case, or just simply follow the logic: don't draw at all if half start, since that means you drawn already!
Problem: way too much variance; really hard to control the length of playout. If playout shuts down midway then it's worthless, so even with a short time limit, a playout still could take really really long.

Topic: Integrating MCTS in imperfect-information, sequential-step games. 

Draw-only MCTS: uses MCTS for the draw policy assuming a random discard policy during playout simulation, and a random policy for the discard policy.
Bilinear MCTS: uses MCTS for the draw policy first assuming a random discard policy during playout simulation, and a subsequent MCTS discard policy assuming a random draw policy during playout simulation. 

Tests:
4 agents, indexed 0 (random), 1 (heuristic), 2 (draw-only MCTS), 3 (bilinear MCTS)
Vs. random agent is trivial. Every agent beats random agent by at least 90%+. For the interest of time, the results won't be shown here. Random agent is primarily used for tuning and benchmarking. (we can run it for like 10000 games as a quick example)
Tests: Heuristic vs Draw-only (time = 0.1), Heuristic vs Bilinear (time = 0.1), Draw-only vs Bilinear (time = 0.1).  [should have enough time overnight, else tune to 0.05]
Optional: if we replace the 'random policies' during playout simulation with actual heuristic policies; would they perform better? especially the bilinear one. (literally copy pasta, or inherit into a new class and overwrite).

Writeup layout:
Time log
Approaches and notes
File descriptions
Work/project contributions
Result comparison and discussion
Potential improvements
Work afterthoughts.
